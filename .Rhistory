library(readxl)
library(caTools)
library(caret)
library(ROCR)
library(corrplot)
library(tidyverse) # for data manipulation and viz
library(naniar)# for missing values
library(reshape2)
library(scales)
library(DataExplorer)
library(questionr)
library(Hmisc)
library(moments)
library(car)
library(recipes)
library(ggplot2)
library(forecast)
library(psych)
#Setting up the directory
setwd("~/GREAT_LEARNING/Capstone Projects/4. Facebook Comment Volume Prediction")
#Reading the data files in R
dataset=read_excel("Training.xlsx", sheet = 1, col_names = TRUE)
names(dataset)<-make.names(names(dataset),unique = TRUE)
#Removing constant and id variable
dataset<-dataset[,-c(1,39)]
#fixing categorical variable
dataset$Page.Category<-as.factor(dataset$Page.Category)
rec<-recipe(~Page.Category, data=dataset)
rec <- rec %>%
step_other(Page.Category, threshold = 0.01, other = "Others")
rec <- prep(rec, training = dataset)
collapsed<- bake(rec, dataset)
dataset$Page.Category<-collapsed$Page.Category
#summary(dataset$Feature.25)
#dataset$Feature.25<-dataset$Feature.25+1366
#for (variable in names(dataset)) {
#  if(any(dataset[,variable]<0)){
#    dataset[,variable]<-dataset[,variable] + min(dataset[,variable])
#  }
#}
#Transforming variables
#Normality of Data
##Transformations
#If lambda = -2.5 to -1.5, inverse square (1/y2)
#If lambda = -2.5 to -0.75, reciprocal (1/y)
#If lambda = -0.75 to -0.25, inverse square root (1/sqrt(y))
#If lambda = -0.25 to 0.25, natural log
#If lambda = 0.25 to 0.75, square root (sqrt(y))
#If lambda = 0.75 to 1.5, none
#If lambda = 1.5 to 2.5, square (y2)
#calculating transforming parameters
#BNobject<-bestNormalize(norm.train$Page.likes, allow_orderNorm = FALSE)
normModel<-preProcess(dataset[,-c(4,39,40)], method="YeoJohnson")
#transform the dataset using the parameters
norm.dataset<-predict(normModel, dataset[,-c(4,39,40)])
norm.dataset<-cbind(dataset[,c(4,39,40)], norm.dataset)
# set capping and floor to the data as per 1st and 99th percentile
norm.dataset[,-c(1:3)] <- norm.dataset[,-c(1:3)] %>%
transmute_all(funs(squish(., quantile(.,c(0.01,0.99), na.rm = TRUE))))
#Now Feature.15 has become a constant variable, we shall remove it
norm.dataset<-norm.dataset[,-17]
# Now impute missing values. we will use aregImpute to impute data
fmla <- as.formula(paste(" ~ ", paste(colnames(norm.dataset), collapse=" +")))
impute_arg <- aregImpute(fmla, data = norm.dataset , n.impute = 5, nk=0)
View(norm.dataset)
View(dataset)
# Get the imputed values
imputed<-impute_arg$imputed
imputed <-impute.transcan(impute_arg, data=norm.dataset,
imputation=5, list.out=TRUE, pr=FALSE, check=FALSE)
# convert the list to the database
imputed.data <- as.data.frame(do.call(cbind,imputed))
# arrange the columns accordingly
imputed.data <- imputed.data[, colnames(norm.dataset), drop = FALSE]
full.data<-imputed.data
full.data$Post.published.weekday<-as.factor(full.data$Post.published.weekday)
full.data$Base.DateTime.weekday<-as.factor(full.data$Base.DateTime.weekday)
full.data[-c(1:3)] <- sapply(full.data[-c(1:3)],as.character)
full.data[-c(1:3)] <- sapply(full.data[-c(1:3)],as.double)
plot(density(na.omit(full.data$Page.Checkins)))
lines(density(full.data$Page.Checkins), col="red")
plot(density(na.omit(full.data$Page.Checkins)))
lines(density(full.data$Page.Checkins), col="red")
plot(density(na.omit(full.data$CC5)))
lines(density(full.data$CC5), col="red")
plot(density(na.omit(normnorm.dataset$Page.Checkins)))
plot(density(na.omit(norm.dataset$Page.Checkins)))
lines(density(full.data$Page.Checkins), col="red")
plot(density(na.omit(norm.dataset$Page.Checkins)), title("Page.Checkins"))
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
plot(density(na.omit(norm.dataset$CC5)), main="CC5")
lines(density(full.data$CC5), col="red")
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend(1, 95, legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:2, cex=0.8)
legend(legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:2, cex=0.8)
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:2, cex=0.8)
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:2, cex=0.8, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:2, cex=0.8, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:1, cex=0.8, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:1, cex=0.8, box.lty = 2, box.lwd = 2)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:1, cex=0.8, box.lty = 2, box.lwd = 1)
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:1, cex=0.8, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:1, cex=0.8, box.lty = 0)
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1:1, cex=0.9, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1, cex=0.9, box.lty = 0)
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1, cex=0.5, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1, cex=0.5, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1, cex=0.6, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1, cex=0.65, box.lty = 0)
plot(density(na.omit(norm.dataset$Page.Checkins)), main="Page.Checkins")
lines(density(full.data$Page.Checkins), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1, cex=0.7, box.lty = 0)
plot(density(na.omit(norm.dataset$CC5)), main="CC5")
lines(density(full.data$CC5), col="red")
legend("topright", legend=c("Original Values", "Predicted Values"),
col=c("black", "red"), lty=1, cex=0.7, box.lty = 0)
#norm.train$Feature.28<- norm.train$Feature.28 - min(norm.train$Feature.28)
#Removing multicollinearity
modelData<-full.data[,-c(1:3,40)]
#Correlation Matrix to check correlation among variables
cor.mat<-cor(modelData)
#Bartlett Test to check if data is appropriate for FA
cortest.bartlett(cor.mat, nrow(modelData))
KMO(modelData)
# remove alias variables
#modelData<-full.data[,-c(1:3,9,10,11,14,16,18,21,23,24,25,30,34)]
model=lm(Target.Variable~.,
data = modelData)
#Calculating Eigen values
ev = eigen(cor(modelData)) # get eigenvalues
EigenValue=ev$values
#Scree plot
Factor=c(1:36)
Scree=data.frame(Factor,EigenValue)
plot(Scree,main="Scree Plot", col="Blue", ylim=c(0,3))
lines(Scree,col="Red")
#Unrotate factors
Unrotate=principal(modelData, nfactors=6, rotate="none")
print(Unrotate,digits=3)
#Rotating factors
Rotate=principal(modelData,nfactors=6,rotate="varimax")
print(Rotate,digits=3)
#Diagram of factors and variables
fa.diagram(Rotate$loadings,simple = FALSE)
#Storing factor scores with new factor names
newData<-as.data.frame(factor.scores(full.data[,-c(1:3,40)], f=Rotate$loadings,  method = "Harman" )$scores)
newData<-cbind(full.data[,c(1:3,40)],newData)
#Creating dummy variables
library(fastDummies)
newData <- dummy_cols(newData,
select_columns = c("Page.Category", "Post.published.weekday", "Base.DateTime.weekday")
,remove_first_dummy = TRUE)
newData<-newData[,-c(1:3)]
#Paritioning the data into training and test dataset
set.seed(2000)
#?sample
n=nrow(newData)
split= sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.70, 0.30))
ptrain = newData[split, ]
ptest = newData[!split,]
fmla <- as.formula(paste("Target.Variable ~ ", paste(colnames(ptrain[,-1]), collapse=" +")))
linearmod<-lm(formula = fmla, data = ptrain)
summary(linearmod)
ptest$lmPred <- predict(linearmod, ptest)
lmactuals_preds<-data.frame(cbind(actuals=ptest$Target.Variable, predicteds=ptest$lmPred))
lmcorrelation_accuracy <- cor(lmactuals_preds)  # 82.7%
library(Metrics)
rmsle(lmactuals_preds$actuals, lmactuals_preds$predicteds)
DMwR::regr.eval(lmactuals_preds$actuals, lmactuals_preds$predicteds)
# XGBoost works with matrices that contain all numeric variables
# we also need to split the training data and label
library(xgboost)
train.mtx<-as.matrix(ptrain[,-1])
label.train<-as.matrix(ptrain[,1])
test.mtx<-as.matrix(ptest[,-1])
xgb.fit <- xgboost(
data = train.mtx,
label = label.train,
eta = 0.1,#this is like shrinkage in the previous algorithm
max_depth = 7,#Larger the depth, more complex the model; higher chances of overfitting. There is no standard                      value for max_depth. Larger data sets require deep trees to learn the rules from data.
min_child_weight = 3,#it blocks the potential feature interactions to prevent overfitting
nrounds = 400,#controls the maximum number of iterations. For classification, it is similar to the number of                       trees to grow.
nfold = 3,
objective = "reg:linear",  # for regression models
verbose = 0,               # silent,
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
#test.mtx<-as.data.frame(test.mtx)
xgb.pred <- predict(xgb.fit, test.mtx)
#Paritioning the data into training and test dataset
set.seed(2000)
#?sample
n=nrow(newData)
split= sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.70, 0.30))
ptrain = newData[split, ]
ptest = newData[!split,]
# XGBoost works with matrices that contain all numeric variables
# we also need to split the training data and label
library(xgboost)
train.mtx<-as.matrix(ptrain[,-1])
label.train<-as.matrix(ptrain[,1])
test.mtx<-as.matrix(ptest[,-1])
xgb.fit <- xgboost(
data = train.mtx,
label = label.train,
eta = 0.1,#this is like shrinkage in the previous algorithm
max_depth = 7,#Larger the depth, more complex the model; higher chances of overfitting. There is no standard                      value for max_depth. Larger data sets require deep trees to learn the rules from data.
min_child_weight = 3,#it blocks the potential feature interactions to prevent overfitting
nrounds = 400,#controls the maximum number of iterations. For classification, it is similar to the number of                       trees to grow.
nfold = 3,
objective = "reg:linear",  # for regression models
verbose = 0,               # silent,
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
#test.mtx<-as.data.frame(test.mtx)
xgb.pred <- predict(xgb.fit, test.mtx)
table(sign(xgb.pred))
xgbfitactuals_preds<-data.frame(cbind(actuals=ptest$Target.Variable, predicteds=xgb.pred))
#fitactuals_preds <- data.frame(cbind(actuals=exp(ptest$Target.Variable), predicteds=exp(fitPred)))  # make actuals_predicteds dataframe.
xgbcorrelation_accuracy <- cor(xgbfitactuals_preds)  # 82.7%
rmsle(xgbfitactuals_preds$actuals, xgbfitactuals_preds$predicteds)
DMwR::regr.eval(xgbfitactuals_preds$actuals, xgbfitactuals_preds$predicteds)
rmsle(xgbfitactuals_preds$actuals, xgbfitactuals_preds$predicteds)
library(randomForest)
library(foreach)
library(doParallel)
nb_trees<-34 #this changes with each test, see table below
nb_cores <-12 #this changes with each test, see table below
cl <- makeCluster(nb_cores)
registerDoParallel(cl)
ptm <- proc.time()
fit <- foreach(ntree = rep(nb_trees, nb_cores), .combine = combine, .packages = "randomForest") %dopar% {
randomForest(y=ptrain[,1],
x=ptrain[,-1],
ntree = ntree,
do.trace=TRUE)}
proc.time() - ptm
stopCluster(cl)
## Scoring syntax
fitPred <- predict(fit, ptest)
table(sign(fitPred))
fitactuals_preds<-data.frame(cbind(actuals=ptest$Target.Variable, predicteds=fitPred))
#fitactuals_preds <- data.frame(cbind(actuals=exp(ptest$Target.Variable), predicteds=exp(fitPred)))  # make actuals_predicteds dataframe.
fitcorrelation_accuracy <- cor(fitactuals_preds)  # 82.7%
# Min-Max Accuracy Calculation
fitmin_max_accuracy <- mean(apply(fitactuals_preds, 1, min) / apply(fitactuals_preds, 1, max))
DMwR::regr.eval(fitactuals_preds$actuals, fitactuals_preds$predicteds)
# => 41.52%, mean absolute percentage deviation
rmsle(fitactuals_preds$actuals, fitactuals_preds$predicteds)
fit
summary(fit)
xgb.importance(feature_names = colnames(train.mtx), model = xgb.fit)
print(xgb.plot.importance(importance_matrix = xgb.importance(feature_names = colnames(train.mtx),
model = xgb.fit)))
print(xgb.plot.importance(importance_matrix = xgb.importance(feature_names = colnames(train.mtx),
model = xgb.fit), top_n =10 ))
print(xgb.plot.importance(importance_matrix = xgb.importance(feature_names = colnames(train.mtx),
model = xgb.fit), top_n =5 ))
print(xgb.plot.importance(importance_matrix = xgb.importance(feature_names = colnames(train.mtx),
model = xgb.fit), top_n =6 ))
# remove alias variables
#modelData<-full.data[,-c(1:3,9,10,11,14,16,18,21,23,24,25,30,34)]
model=lm(Target.Variable~.,
data = modelData)
#norm.train$Feature.28<- norm.train$Feature.28 - min(norm.train$Feature.28)
#Removing multicollinearity
modelData<-full.data[,-c(1:3,40)]
# remove alias variables
#modelData<-full.data[,-c(1:3,9,10,11,14,16,18,21,23,24,25,30,34)]
model=lm(Target.Variable~.,
data = modelData)
summary(model)
View(full.data)
#norm.train$Feature.28<- norm.train$Feature.28 - min(norm.train$Feature.28)
#Removing multicollinearity
modelData<-full.data[,-c(1:3)]
# remove alias variables
#modelData<-full.data[,-c(1:3,9,10,11,14,16,18,21,23,24,25,30,34)]
model=lm(Target.Variable~.,
data = modelData)
summary(model)
full.data<-imputed.data
full.data$Post.published.weekday<-as.factor(full.data$Post.published.weekday)
full.data$Base.DateTime.weekday<-as.factor(full.data$Base.DateTime.weekday)
full.data[-c(1:3)] <- sapply(full.data[-c(1:3)],as.character)
full.data[-c(1:3)] <- sapply(full.data[-c(1:3)],as.double)
#norm.train$Feature.28<- norm.train$Feature.28 - min(norm.train$Feature.28)
#Removing multicollinearity
modelData<-full.data[,-c(1:3)]
# remove alias variables
#modelData<-full.data[,-c(1:3,9,10,11,14,16,18,21,23,24,25,30,34)]
model=lm(Target.Variable~.,
data = modelData)
summary(model)
View(dataset)
#norm.train$Feature.28<- norm.train$Feature.28 - min(norm.train$Feature.28)
#Removing multicollinearity
modelData<-dataset[,-c(4,39,40)]
# remove alias variables
#modelData<-full.data[,-c(1:3,9,10,11,14,16,18,21,23,24,25,30,34)]
model=lm(Target.Variable~.,
data = modelData)
summary(model)
full.data
#norm.train$Feature.28<- norm.train$Feature.28 - min(norm.train$Feature.28)
#Removing multicollinearity
modelData<-full.data[,-c(1:3)]
# remove alias variables
#modelData<-full.data[,-c(1:3,9,10,11,14,16,18,21,23,24,25,30,34)]
model=lm(Target.Variable~.,
data = modelData)
summary(model)
print(Rotate,digits=3)
ggplot(dataset, aes(x=Page.Category)) + geom_histogram()
ggplot(dataset, aes(x=Page.Category)) + geom_histogram(stat = "count")
